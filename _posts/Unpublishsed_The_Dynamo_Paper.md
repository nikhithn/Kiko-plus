## Consistent Hashing - Partitioning, Replication and Failure Handling
DynamoDB is a key-value store, so it is essentially a distributed, persistent hash table.  Key operations for a distributed hash-table include ensuring that failures and additions of new servers to the hash-table is not computationally expensive. 

Let's look at a naive hash algorithm.  Suppose you have a hash function that takes any input and converts it to an integer.  You can distribute the hash table over n servers by computing hash(input) % n.  The output is necessarily between 0 and n-1, and this can then map to the server and inform where to store the information.  Assuming that the distribution of the hash is uniform (at least over %n) you will have an equal distribution of data across each server.  If some servers can take more load than others, you can increase the range of hash % n that you assign to that server.  For example, if a server can take 2x as much load as the average, you can assign 2 integers to the server, and then take the hash % n+1 of the value.  Then the server will receive 2x as much load as others.

The naive algorithm provides the lookup and write performance and distribution that we need; however, it does not provide the incremental scalability that we need.  To see this, suppose we add a server to the distributed hash-table.  Now, we have n+1 servers, so our assignment function will have to be updated to hash % n+1 to determine the server that will store the input.  However, this would require us to update existing maps, since hash % n+1 would result in a different value for all inputs.  So, we would need to move all of our data around when we add a new hash.  There are ways to improve this, but they all result in at least n/(n+1) data being moved.  

The consistent hashing algorithm helps resolve this.  Consistent hashing takes a slightly different approach to determining which node will store the input.  With consistent hashing, we can view the hash value as continuous over some range, say 0 to n.  We calculate the hash value and then place the input into the first node that is greater than the hash value.  In this case, if there is failure at node n-1, all of the hashes would simply map to n instead of n-1 (since n would be the new "first node that is greater than the hash value").  So, moving the data would be quicker if a node were added or deleted, since only 1/n of the data would need to be moved.  However, of course, this would lead to the problem that different nodes could have different quantities of data.  For example, if node n-1 failed, n would now have to serve 2x as much load.  This can be resolved by creating multiple points on the continuous hash function that maps to each server.  For example, if all node re randomly placed on the output of the hash function at 5-6 different locations (each), at each location, the subsequent node would likely be different.  So, if a node went down, the load would be distributed between each of the subsequent nodes, and the distribution would be more fair.  We would still only need to copy each piece of data once, so there would still be only 1/n data copied.  

Lastly, we can easily handle replication in consistent hashing by just writing to the next r nodes after the hash value where r is the number of replicas.  In this case, if a node goes down, the data would be maintained, and also already be located where consistent hashing would expect it to be located (at the next node).  Then, copying the data to an additional replica can occur as a background process to maintain r copies.  

## Vector Clocks & Quorums - Consistency 
